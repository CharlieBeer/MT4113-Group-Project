% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/FunOptim.R
\name{funoptim}
\alias{funoptim}
\title{Parent Optimisation Function}
\usage{
funoptim(
  f,
  inits = NULL,
  data = NULL,
  minimum = TRUE,
  tol = 1e-08,
  maxit = 100,
  method = NULL,
  gradfn = NULL,
  hessfn = NULL,
  jacobfn = NULL
)
}
\arguments{
\item{f}{Function to be optimised}

\item{inits}{Initial parameter values}

\item{data}{(optional) Dataframe.}

\item{minimum}{(optional) Search for minimum or maximum (takes TRUE if minimum search, FALSE if maximum search)}

\item{tol}{(optional) Tolerance level - defaults to 1e-8}

\item{maxit}{(optional) Maximum number of iterations run before stopping - defaults to 100}

\item{method}{optimisation method, takes one of "GS", "BS", "UVN", "MVN", "GN"}

\item{gradfn}{(optional) A gradient function}

\item{hessfn}{(optional) A hessian function}

\item{jacobfn}{(optional) A jacobian function}
}
\value{
A list containing:
\item{estimate}{Optimised estimate}
\item{feval}{Function evaluated at optimised estimate}
\item{grad}{Gradient of function at optimised estimate}
\item{tolerance}{Tolerance level reached through optimisation}
\item{conv}{Whether or not the optimisation converged. 0 - converged, 1 - did not converge, 2 - max iterations reached}
\item{niter}{Number of iterations run}
}
\description{
Performs a variety of optimisation methods based on the 'method' input.
}
\examples{
# Multivariate Newton (MVN)
f <- function(theta){
 x <- theta[1]
 y <- theta[2]
 return((x-3)^2 + (y+2)^2)
}
funoptim(f = f, inits = c(4, -1),method="MVN")
}
\references{
Swallow, B. (2025). Univariate Optimization.
University of St Andrews.
\url{https://moody.st-andrews.ac.uk/moodle/pluginfile.php/2128841/mod_resource/content/2/Chpater7_12.pdf}
}
\author{
Charlie Beer, Holly Goldsmith, Ilana Goldman, Chenyu Lin
}
